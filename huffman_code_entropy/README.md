# HuffmanÂ Coding & Entropy Analysis  
*(folderÂ `huffman_code_entropy/`)*


In short, we

1. estimate **symbol probabilities** (lettersâ€¯+â€¯space) for each text,  
2. **visualise** the distributions,  
3. compute the **entropy**Â H of each distribution,  
4. build an **optimal instantaneous binary code**Â C for the *first* text
   using the **Huffman algorithm**,  
5. measure the **average codeâ€‘word length** ğ¿(C) for *both* texts and
   discuss optimality.

The whole solution lives in a single Jupyter notebook:

```
huffman_code_entropy/
â”œâ”€â”€ 001.txt                    â† first Czech prose excerpt
â”œâ”€â”€ 011.txt                    â† second Czech prose excerpt
â”œâ”€â”€ huffman_code_entropy.ipynb â† complete analysis & plots
â””â”€â”€ README.md                  â† you are here
```

---

## 1â€‚QuickÂ Start

```bash
# (1) create & activate a fresh env (recommended)
python -m venv .venv && source .venv/bin/activate

# (2) install required packages
pip install jupyter pandas matplotlib

# (3) open the notebook
jupyter lab huffman_code_entropy.ipynb
```

Run **all cells** â€“ execution takes \<Â 1â€¯s.

---

## 2â€‚What the Notebook Does

| Section | Code highlights | Outputs |
|---------|-----------------|---------|
| **Load & preprocess** | `Counter`, lowercase mapping, keep **lettersÂ aâ€“z + space**. | Text length & filtered length stats. |
| **Probability estimation** | Normalise symbol counts. | Two bar plots (probability vs. symbol). |
| **Entropy** | `H = âˆ’Î£â€¯páµ¢â€¯logâ‚‚â€¯páµ¢` for each text. | Prints `Hâ‚`, `Hâ‚‚` inÂ bits / symbol. |
| **Huffman coding** | Classic priorityâ€‘queue implementation (`Leaf`, `Node`). | Table `char â†’ code`, prefixâ€‘free. |
| **Average length** | `ğ¿ = Î£â€¯páµ¢â€¯Â·â€¯|codeáµ¢|` for *C* on textÂ 1 and **also** on textÂ 2. | ğ¿(C,Â textÂ 1) and ğ¿(C,Â textÂ 2). |
| **Optimality check** | Shows `ğ¿ âˆ’ H` gaps & explains why Huffman is minimal. | Commentary cell with justification. |

---

## 3â€‚Why the CodeÂ C is Optimal

The Huffman algorithm constructs a **prefixâ€‘free (instantaneous) code**
whose average length ğ¿ satisfies  

> **H â‰¤ ğ¿ < Hâ€¯+â€¯1**

and is **provably minimal** among all binary prefix codes for the given
probability vector.  
Because our implementation uses the canonical algorithm (repeatedly
merging the two leastâ€‘probable symbols), the resulting code *must* be
optimal for the **first text**.

For the **second text** we reuse the same code and observe  
`Î” = ğ¿(C,Â textâ€¯2)Â âˆ’Â Hâ‚‚Â >Â 0`.  
If Î” is close to 1â€¯bit we know we can build a shorter code tailored to
the second distribution; hence C is **not** optimal there.

---

## 4â€‚License &Â Credits

Released under the **MIT License** â€“ feel free to use, modify and
redistribute.  If you publish derived work, please cite this repo.
